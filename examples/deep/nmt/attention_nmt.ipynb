{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attention_nmt.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxYOQbY85DHu",
        "colab_type": "text"
      },
      "source": [
        "# NMT Example --- Traditional ML Approach\n",
        "\n",
        "Adapted from https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
        "\n",
        "## Set Up Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyAXZf8VLu5T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "97938d44-5621-4ff9-8de8-587d8576c97f"
      },
      "source": [
        "!pip install --upgrade https://storage.googleapis.com/jax-releases/cuda101/jaxlib-0.1.51-cp36-none-manylinux2010_x86_64.whl\n",
        "!pip install --upgrade jax\n",
        "!pip install git+https://github.com/deepmind/dm-haiku\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax.experimental import optimizers\n",
        "\n",
        "import haiku as hk\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: jax in /usr/local/lib/python3.6/dist-packages (0.1.75)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from jax) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.6/dist-packages (from jax) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum in /usr/local/lib/python3.6/dist-packages (from jax) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from absl-py->jax) (1.15.0)\n",
            "Collecting git+https://github.com/deepmind/dm-haiku\n",
            "  Cloning https://github.com/deepmind/dm-haiku to /tmp/pip-req-build-n8f8jxjj\n",
            "  Running command git clone -q https://github.com/deepmind/dm-haiku /tmp/pip-req-build-n8f8jxjj\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from dm-haiku==0.0.2) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.6/dist-packages (from dm-haiku==0.0.2) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py>=0.7.1->dm-haiku==0.0.2) (1.15.0)\n",
            "Building wheels for collected packages: dm-haiku\n",
            "  Building wheel for dm-haiku (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dm-haiku: filename=dm_haiku-0.0.2-cp36-none-any.whl size=289739 sha256=31a1f3bf7c0bc62f063c1630283257dac52b38679e60d2ef754b5cf2192cf32c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-0la00c1v/wheels/97/0f/e9/17f34e377f8d4060fa88a7e82bee5d8afbf7972384768a5499\n",
            "Successfully built dm-haiku\n",
            "Installing collected packages: dm-haiku\n",
            "Successfully installed dm-haiku-0.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgl1qQ_CNMVV",
        "colab_type": "text"
      },
      "source": [
        "## Dataset Processing & NLP-specific Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQ2Q5nr0MCt2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "0f187b18-b5a9-4dad-f7b9-7c3e997aea86"
      },
      "source": [
        "# Download the file\n",
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
        "    extract=True)\n",
        "\n",
        "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2646016/2638744 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6u6RLNsMfCP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ========= DATA PROCESSING =============\n",
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "  w = w.strip()\n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w\n",
        "\n",
        "# 1. Remove the accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
        "def create_dataset(path, num_examples):\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "\n",
        "  return zip(*word_pairs)\n",
        "\n",
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer\n",
        "\n",
        "def load_dataset(path, num_examples=None):\n",
        "  # creating cleaned input, output pairs\n",
        "  targ_lang, inp_lang = create_dataset(path, num_examples)\n",
        "\n",
        "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ru2m3ZMbM_W6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4abc7f1f-3ce2-43da-ab63-a8a26ad5a2ca"
      },
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "num_examples = 30000\n",
        "# num_examples = -1\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
        "\n",
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24000 24000 6000 6000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOrNnUfHNiX7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4de4fa17-6657-4dee-d41d-a97ec97f413e"
      },
      "source": [
        "#make the dataset\n",
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "dataset = tfds.as_numpy(dataset)\n",
        "\n",
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((64, 16), (64, 11))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-i6hvE5NwBN",
        "colab_type": "text"
      },
      "source": [
        "## Define the Encoder-Decoder Model\n",
        "\n",
        "This is a standard encoder-decoder architecture with attentional decoding. See the paper https://arxiv.org/pdf/1409.0473.pdf for details. The attention mechanism allows the model to selectively *attend* to the encoded inputs, allowing the model to focus on the most important inputs in the source language for each prediction in the target language.\n",
        "\n",
        "We use a GRU-based recurrent model with scaled dot-product attention (which is different from the paper above). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9p8N9KJUN80w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(hk.Module):\n",
        "  def __init__(self, vocab_size, d_model):\n",
        "    super(Encoder, self).__init__()\n",
        "    #is it better to keep the embedding outside?\n",
        "    self.embedding = hk.Embed(vocab_size=vocab_size, embed_dim=d_model)\n",
        "    self.gru = hk.GRU(hidden_size=d_model, \n",
        "                      w_i_init=hk.initializers.VarianceScaling(1.0, \"fan_avg\", \"uniform\"),\n",
        "                      w_h_init=hk.initializers.VarianceScaling(1.0, \"fan_avg\", \"uniform\"),\n",
        "                      b_init=hk.initializers.Constant(0.0))\n",
        "  \n",
        "  def initial_state(self, batch_size):\n",
        "    return self.gru.initial_state(batch_size)\n",
        "  \n",
        "  def __call__(self, tokens, init_state):\n",
        "    inputs = self.embedding(tokens)\n",
        "    return hk.dynamic_unroll(self.gru, inputs, init_state)\n",
        "\n",
        "class ScaledDotAttention(hk.Module):\n",
        "  \"\"\" Implements single-headed scaled dot-product attention \"\"\" \n",
        "  def __init__(self, d_model):\n",
        "    super(ScaledDotAttention, self).__init__()\n",
        "    self.W_Q = hk.Linear(d_model, w_init = hk.initializers.VarianceScaling(1.0, \"fan_avg\", \"uniform\"),\n",
        "                                  b_init = hk.initializers.Constant(0.0))\n",
        "    self.W_K = hk.Linear(d_model, w_init = hk.initializers.VarianceScaling(1.0, \"fan_avg\", \"uniform\"),\n",
        "                                  b_init = hk.initializers.Constant(0.0))\n",
        "    self.W_V = hk.Linear(d_model, w_init = hk.initializers.VarianceScaling(1.0, \"fan_avg\", \"uniform\"),\n",
        "                                  b_init = hk.initializers.Constant(0.0))\n",
        "    self.d_model = d_model\n",
        "    self.root_d_model = np.sqrt(self.d_model)\n",
        "  \n",
        "  def __call__(self, Q, K, V):\n",
        "    #apply linear projections to the Queries, Keys, and Values\n",
        "    Q = self.W_Q(Q)\n",
        "    K = self.W_K(K)\n",
        "    V = self.W_V(V)\n",
        "\n",
        "    #batch-dimension last...this is weird\n",
        "    scores = jnp.einsum('...bd,tbd->...tb', Q, K)/self.root_d_model\n",
        "\n",
        "    #normalize the scores\n",
        "    probs = jax.nn.softmax(scores, axis=-2)\n",
        "    \n",
        "    #average the values w.r.t. the probs\n",
        "    return  jnp.einsum('...tb,tbd->...bd', probs, V)\n",
        "    \n",
        "class BhadanauAttention(hk.Module):\n",
        "  def __init__(self, d_model):\n",
        "    super(BhadanauAttention, self).__init__()\n",
        "    self.W_Q = hk.Linear(d_model, w_init = hk.initializers.VarianceScaling(1.0, \"fan_avg\", \"uniform\"),\n",
        "                                  b_init = hk.initializers.Constant(0.0))\n",
        "    self.W_K = hk.Linear(d_model, w_init = hk.initializers.VarianceScaling(1.0, \"fan_avg\", \"uniform\"),\n",
        "                                  b_init = hk.initializers.Constant(0.0))\n",
        "    self.W_score = hk.Linear(1, w_init = hk.initializers.VarianceScaling(1.0, \"fan_avg\", \"uniform\"),\n",
        "                                  b_init = hk.initializers.Constant(0.0))\n",
        "\n",
        "  def __call__(self, Q, K, V):\n",
        "    Q = jnp.expand_dims(Q, 0)\n",
        "    #project the inputs\n",
        "    Q = self.W_Q(Q)\n",
        "    K = self.W_K(K)\n",
        "\n",
        "    # compute the scores using the Bhadanau attention mechanism\n",
        "    scores = self.W_score(jnp.tanh(Q + K))\n",
        "\n",
        "    # normalize the scores into probs\n",
        "    probs = jax.nn.softmax(scores, axis=0) #0 is time axis\n",
        "\n",
        "    # average the values w.r.t. the probs\n",
        "    return jnp.einsum('tbd,tbd->bd', probs, V)\n",
        "\n",
        "class Decoder(hk.Module):\n",
        "  def __init__(self, attn, vocab_size, d_model):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.embedding = hk.Embed(vocab_size=vocab_size, embed_dim=d_model)\n",
        "    self.attn = attn\n",
        "    self.gru = hk.GRU(hidden_size=d_model, \n",
        "                      w_i_init=hk.initializers.VarianceScaling(1.0, \"fan_avg\", \"uniform\"),\n",
        "                      w_h_init=hk.initializers.VarianceScaling(1.0, \"fan_avg\", \"uniform\"),\n",
        "                      b_init=hk.initializers.Constant(0.0))\n",
        "    \n",
        "    self.proj = hk.Linear(vocab_size)\n",
        "  \n",
        "  def initial_state(self, batch_size):\n",
        "    return self.gru.initial_state(batch_size)\n",
        "\n",
        "  def __call__(self, tokens, enc_outputs, hidden_state):\n",
        "    \"\"\" do attention with queries = hidden state, keys = enc_outputs, \n",
        "        values = enc_outputs to select the most 'relevant' encoded outputs \n",
        "        to the hidden state.\"\"\"\n",
        "    \n",
        "    # hidden_state = np.expand_dims(hidden_state, 0)\n",
        "    ctx_vector = self.attn(hidden_state, enc_outputs, enc_outputs)\n",
        "\n",
        "    # embed the tokens with the target embedding\n",
        "    inputs = self.embedding(tokens)\n",
        "\n",
        "    # concat the ctx_vector to the embeddings\n",
        "    inputs = jnp.concatenate([ctx_vector, inputs], axis=-1)\n",
        "\n",
        "    #apply the decoder to the context + inputs\n",
        "    outputs, hidden_state = self.gru(inputs, hidden_state)\n",
        "\n",
        "    # project outputs into logit space and return (logits, hidden_state)\n",
        "    return self.proj(outputs), hidden_state\n",
        "  "
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjNxAYsGOyHH",
        "colab_type": "text"
      },
      "source": [
        "## Define the Encoder and Decoder 'Forward' functions\n",
        "\n",
        "We define these separately since we need to run the encoder once and the decoder multiple times for autoregressive decoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OVEJ2RDQ3Rw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encoder_fn(input_seqs):\n",
        "  \"\"\" assumes input_seqs is time-first\n",
        "      args:\n",
        "        input_seqs: an input sequence of tokens\n",
        "      \n",
        "      returns:\n",
        "         a tuple of arrays for the the encoded outputs and the final hidden state of the encoder \n",
        "  \"\"\"\n",
        "  \n",
        "  encoder = Encoder(vocab_size=vocab_inp_size, d_model = embedding_dim)\n",
        "  batch_size = input_seqs.shape[1]\n",
        "\n",
        "  #initialize the hidden state\n",
        "  enc_initial_state = encoder.initial_state(batch_size)\n",
        "  \n",
        "  #apply the encoder to the full sequence using hk.dynamic_unroll(...)\n",
        "  enc_outputs, enc_hidden = encoder(input_seqs, enc_initial_state)\n",
        "\n",
        "  return enc_outputs, enc_hidden\n",
        "\n",
        "def decoder_fn(dec_inputs, hidden_state, enc_outputs):\n",
        "  \"\"\" assumes dec_inputs are time-first \"\"\"\n",
        "  attn = ScaledDotAttention(d_model = embedding_dim)\n",
        "  # attn = BhadanauAttention(d_model = embedding_dim) # uncomment for Bhadanau attention\n",
        "  \n",
        "  decoder = Decoder(attn, vocab_size = vocab_tar_size, d_model = embedding_dim)\n",
        "\n",
        "  # apply the decoder to a single input (i.e. not unrolled) since we need \n",
        "  # to autoregressively generate the translation.\n",
        "  outputs, hidden_state = decoder(dec_inputs, enc_outputs, hidden_state)\n",
        "\n",
        "  return outputs, hidden_state\n",
        "\n",
        "def init_params(key, batch):\n",
        "  test_inputs, test_targets = batch\n",
        "\n",
        "  #transpose inputs to be time-first\n",
        "  test_inputs = test_inputs.transpose(1,0)\n",
        "  test_targets = test_targets.transpose(1,0)\n",
        "\n",
        "  encoder = hk.transform(encoder_fn, apply_rng = True)\n",
        "  enc_params = encoder.init(jax.random.PRNGKey(42), test_inputs)\n",
        "  enc_outputs, enc_hiddens = encoder.apply(enc_params, jax.random.PRNGKey(0), test_inputs)\n",
        "\n",
        "  decoder = hk.transform(decoder_fn, apply_rng = True)\n",
        "  dec_params = decoder.init(jax.random.PRNGKey(42), test_targets[0], \n",
        "                            enc_hiddens, enc_outputs)\n",
        "\n",
        "  return enc_params, dec_params"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFL1BomTPLMX",
        "colab_type": "text"
      },
      "source": [
        "## Define the Loss Function and Train Step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-x8FbdUTHk0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 1e-3\n",
        "opt_init, opt_update, opt_get_params = optimizers.adam(lr)\n",
        "\n",
        "def masked_crossent(logits, targets):\n",
        "  one_hot_labels = jax.nn.one_hot(targets, vocab_tar_size)\n",
        "\n",
        "  # since we have padded the batch with 0s, to make them uniform length, \n",
        "  # we need to mask out the padding tokens, which are index-zero tokens\n",
        "  mask = jnp.expand_dims(targets > 0,1)\n",
        "\n",
        "  #do masked mean, ensuring that length-zero batches don't give nan.\n",
        "  denom = jnp.max(jnp.array([jnp.sum(mask), 1]))\n",
        "  crossent = - jnp.sum(one_hot_labels * jax.nn.log_softmax(logits) * mask) / denom\n",
        "\n",
        "  return crossent\n",
        "\n",
        "def loss(params, batch):\n",
        "  enc_params, dec_params = params\n",
        "\n",
        "  input_batch, target_batch = batch\n",
        "\n",
        "  #transpose batch to be time-first\n",
        "  input_batch = jnp.transpose(input_batch, (1,0))\n",
        "  target_batch = jnp.transpose(target_batch, (1,0))\n",
        "\n",
        "  #encode the batch once\n",
        "  enc_outputs, enc_hidden = encoder.apply(enc_params, jax.random.PRNGKey(0), input_batch)\n",
        "\n",
        "  #initalize the decoder's hidden state to be the encoder's hidden state\n",
        "  dec_hidden = enc_hidden\n",
        "\n",
        "  #start predicting with the <start> token\n",
        "  dec_input = jnp.array([targ_lang.word_index['<start>']] * BATCH_SIZE)\n",
        "\n",
        "  t_max = target_batch.shape[0]\n",
        "  loss = 0.0\n",
        "  for t in range(1, t_max):\n",
        "    # iterate through the targets\n",
        "    targets = target_batch[t]\n",
        "\n",
        "    # compute logits over target vocabulary for the current word (targets)\n",
        "    logits, dec_hidden = decoder.apply(dec_params, jax.random.PRNGKey(0), \n",
        "                                       dec_input, dec_hidden, enc_outputs)\n",
        "\n",
        "    # accumulate the loss\n",
        "    loss += masked_crossent(logits, targets)\n",
        "    \n",
        "    # use teacher forcing by providing the ground-truth input to the model at each timestep\n",
        "    dec_input = targets\n",
        "\n",
        "  return loss / t_max\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def train_step(i, opt_state, batch):\n",
        "    params = opt_get_params(opt_state)\n",
        "    # batch_loss_fn = lambda p: loss(p, batch)\n",
        "    # fx, dx = jax.value_and_grad(batch_loss_fn)(params)\n",
        "    fx, dx = jax.value_and_grad(loss)(params, batch)\n",
        "    opt_state = opt_update(i, dx, opt_state)\n",
        "    return fx, opt_state\n",
        "\n",
        "\n",
        "def eval_step(params, sentence, max_len=32):\n",
        "  \"\"\" decodes a single input sentence, provided as a string \"\"\"\n",
        "  enc_params, dec_params = params\n",
        "\n",
        "  # tokenize input string\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "  inputs = [inp_lang.word_index[token] for token in sentence.split(' ')]\n",
        "  inputs = np.expand_dims(jnp.array(inputs), 1)\n",
        "\n",
        "  # encode the inputs\n",
        "  enc_outputs, enc_hidden = encoder.apply(enc_params, jax.random.PRNGKey(0), inputs)\n",
        "\n",
        "  # initialize the decoder's hidden state with the encoder's hidden state\n",
        "  dec_hidden = enc_hidden\n",
        "\n",
        "  #start predicting with the <start> token\n",
        "  dec_input = jnp.array([targ_lang.word_index['<start>']] * 1)\n",
        "\n",
        "  result = []\n",
        "  for t in range(1, max_len):\n",
        "    # compute the logits for the current token\n",
        "    logits, dec_hidden = decoder.apply(dec_params, jax.random.PRNGKey(0), \n",
        "                                       dec_input, dec_hidden, enc_outputs)\n",
        "\n",
        "    # greedy-decode the prediction\n",
        "    pred_idx = int(jnp.argmax(logits))\n",
        "    result.append(targ_lang.index_word[pred_idx])\n",
        "\n",
        "    #if the decoder says 'stop', return\n",
        "    if targ_lang.index_word[pred_idx] == '<end>':\n",
        "      break\n",
        "    \n",
        "    #otherwise, the prediction becomes the input (for autogregressive decoding)\n",
        "    dec_input = jnp.array([pred_idx])\n",
        "  \n",
        "  return \" \".join(result) + '.'"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7daxYUNPRqg",
        "colab_type": "text"
      },
      "source": [
        "## Do the training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylxox6CEjOZS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "cf41a742-5940-4977-c5aa-0b9426212424"
      },
      "source": [
        "init_key = jax.random.PRNGKey(0)\n",
        "params = init_params(init_key, next(dataset))\n",
        "opt_state = opt_init(params)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train))\\\n",
        "                         .shuffle(BUFFER_SIZE)\n",
        "\n",
        "for epoch in range(10):\n",
        "  epoch_loss = 0.0\n",
        "  dataset_iter = tfds.as_numpy(train_dataset.batch(BATCH_SIZE, drop_remainder=True))\n",
        "  \n",
        "  start = time.time()\n",
        "  for b, batch in enumerate(dataset_iter):\n",
        "    train_loss, opt_state = train_step(b, opt_state, batch)\n",
        "    epoch_loss += train_loss\n",
        "  \n",
        "  print(f\"epoch = {epoch}\",\n",
        "        f\" | train loss = {epoch_loss / (b + 1):.5f}\",\n",
        "        f\" | time per epoch = {time.time() - start:.2f}s\")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch = 0  | train loss = 1.84770  | time per epoch = 13.29s\n",
            "epoch = 1  | train loss = 1.17798  | time per epoch = 5.35s\n",
            "epoch = 2  | train loss = 0.92113  | time per epoch = 5.38s\n",
            "epoch = 3  | train loss = 0.74348  | time per epoch = 5.38s\n",
            "epoch = 4  | train loss = 0.60680  | time per epoch = 5.41s\n",
            "epoch = 5  | train loss = 0.50736  | time per epoch = 5.46s\n",
            "epoch = 6  | train loss = 0.42633  | time per epoch = 5.46s\n",
            "epoch = 7  | train loss = 0.34549  | time per epoch = 5.42s\n",
            "epoch = 8  | train loss = 0.28708  | time per epoch = 5.41s\n",
            "epoch = 9  | train loss = 0.24023  | time per epoch = 5.40s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dX4YhirSPTwy",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate on some sample sentences\n",
        "\n",
        "Note: this is a simple model trained on a subset of the data. The translations are not perfect (below are some reasonable outputs)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWxX3fT7nZz-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "2c9f312a-fa34-4703-e46e-f01689ad0498"
      },
      "source": [
        "params = opt_get_params(opt_state)\n",
        "print(eval_step(params, u'hace mucho calor aqui.'))\n",
        "print(eval_step(params, u'hola!'))\n",
        "print(eval_step(params, u'¿cómo estás?'))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "it s hot here . <end>.\n",
            "hello ! <end>.\n",
            "how are you ? <end>.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}