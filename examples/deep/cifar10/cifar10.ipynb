{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLATnUvpdftH",
        "colab_type": "text"
      },
      "source": [
        "# jax-bayes CIFAR10 Example --- Traditional ML Approach\n",
        "\n",
        "## Set up the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfMSaNHlceB7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d04f814f-8140-4a5c-f67a-d3035258bc14"
      },
      "source": [
        "#see https://github.com/google/jax#pip-installation\n",
        "!pip install --upgrade https://storage.googleapis.com/jax-releases/cuda101/jaxlib-0.1.51-cp36-none-manylinux2010_x86_64.whl\n",
        "!pip install --upgrade jax\n",
        "!pip install git+https://github.com/deepmind/dm-haiku\n",
        "!pip install git+https://github.com/jamesvuc/jax-bayes"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting jaxlib==0.1.51\n",
            "\u001b[?25l  Downloading https://storage.googleapis.com/jax-releases/cuda101/jaxlib-0.1.51-cp36-none-manylinux2010_x86_64.whl (71.5MB)\n",
            "\u001b[K     |████████████████████████████████| 71.5MB 42kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from jaxlib==0.1.51) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from jaxlib==0.1.51) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.6/dist-packages (from jaxlib==0.1.51) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from absl-py->jaxlib==0.1.51) (1.15.0)\n",
            "Installing collected packages: jaxlib\n",
            "  Found existing installation: jaxlib 0.1.52\n",
            "    Uninstalling jaxlib-0.1.52:\n",
            "      Successfully uninstalled jaxlib-0.1.52\n",
            "Successfully installed jaxlib-0.1.51\n",
            "Requirement already up-to-date: jax in /usr/local/lib/python3.6/dist-packages (0.1.75)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from jax) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.6/dist-packages (from jax) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum in /usr/local/lib/python3.6/dist-packages (from jax) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from absl-py->jax) (1.15.0)\n",
            "Collecting git+https://github.com/deepmind/dm-haiku\n",
            "  Cloning https://github.com/deepmind/dm-haiku to /tmp/pip-req-build-qx61eemy\n",
            "  Running command git clone -q https://github.com/deepmind/dm-haiku /tmp/pip-req-build-qx61eemy\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from dm-haiku==0.0.2) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.6/dist-packages (from dm-haiku==0.0.2) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py>=0.7.1->dm-haiku==0.0.2) (1.15.0)\n",
            "Building wheels for collected packages: dm-haiku\n",
            "  Building wheel for dm-haiku (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dm-haiku: filename=dm_haiku-0.0.2-cp36-none-any.whl size=289739 sha256=0ea4611f09ee7534f77a37f5f875814f9437bb2aa72d43f19d3b69d4892aabfb\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-gsov__2x/wheels/97/0f/e9/17f34e377f8d4060fa88a7e82bee5d8afbf7972384768a5499\n",
            "Successfully built dm-haiku\n",
            "Installing collected packages: dm-haiku\n",
            "Successfully installed dm-haiku-0.0.2\n",
            "Collecting git+https://github.com/jamesvuc/jax-bayes\n",
            "  Cloning https://github.com/jamesvuc/jax-bayes to /tmp/pip-req-build-tbzmaa7c\n",
            "  Running command git clone -q https://github.com/jamesvuc/jax-bayes /tmp/pip-req-build-tbzmaa7c\n",
            "Requirement already satisfied: absl-py>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from jax-bayes==0.0.1) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.6/dist-packages (from jax-bayes==0.0.1) (1.18.5)\n",
            "Requirement already satisfied: opt-einsum>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from jax-bayes==0.0.1) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.12.4 in /usr/local/lib/python3.6/dist-packages (from jax-bayes==0.0.1) (3.12.4)\n",
            "Collecting scipy>=1.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/a8/f4c66eb529bb252d50e83dbf2909c6502e2f857550f22571ed8556f62d95/scipy-1.5.2-cp36-cp36m-manylinux1_x86_64.whl (25.9MB)\n",
            "\u001b[K     |████████████████████████████████| 25.9MB 117kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from jax-bayes==0.0.1) (1.15.0)\n",
            "Collecting tqdm>=4.48.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/7e/281edb5bc3274dfb894d90f4dbacfceaca381c2435ec6187a2c6f329aed7/tqdm-4.48.2-py2.py3-none-any.whl (68kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.12.4->jax-bayes==0.0.1) (49.2.0)\n",
            "Building wheels for collected packages: jax-bayes\n",
            "  Building wheel for jax-bayes (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jax-bayes: filename=jax_bayes-0.0.1-cp36-none-any.whl size=1009734 sha256=ce9211265ff46056ed79baedb149a7f3d5420fb2b4234ecbcfc5d73695119b9f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-38e51wxr/wheels/31/65/d6/bcf4b5e84c6f6f176e73850145875e806569759c23081b4446\n",
            "Successfully built jax-bayes\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement scipy==1.4.1, but you'll have scipy 1.5.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: scipy, tqdm, jax-bayes\n",
            "  Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "Successfully installed jax-bayes-0.0.1 scipy-1.5.2 tqdm-4.48.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l70DSI0ajQJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import haiku as hk\n",
        "\n",
        "import jax.numpy as jnp\n",
        "from jax.experimental import optimizers\n",
        "import jax\n",
        "\n",
        "import sys, os, math, time\n",
        "import numpy as np\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B686kNdCzFEP",
        "colab_type": "text"
      },
      "source": [
        "## Build the dataset loader and CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzgaUa2owIqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset(split, is_training, batch_size, repeat=True, seed=0):\n",
        "  if repeat:\n",
        "    ds = tfds.load('cifar10', split=split).cache().repeat()\n",
        "  else:\n",
        "    ds = tfds.load('cifar10', split=split).cache()\n",
        "  if is_training:\n",
        "    ds = ds.shuffle(10 * batch_size, seed=seed)\n",
        "  ds = ds.batch(batch_size)\n",
        "  return tfds.as_numpy(ds)\n",
        "\n",
        "# build a 32-32-64-32 CNN with max-pooling \n",
        "# followed by a 128-10-n_classes MLP\n",
        "class Net(hk.Module):\n",
        "  def __init__(self, dropout=0.1, n_classes=10):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv_stage = hk.Sequential([\n",
        "      #block 1\n",
        "      hk.Conv2D(32, kernel_shape=3, stride=1, padding='SAME'), \n",
        "      jax.nn.relu, \n",
        "      hk.MaxPool(window_shape=(1,2,2,1), strides=(1,1,1,1), padding='VALID'),\n",
        "      # block 2\n",
        "      hk.Conv2D(32, kernel_shape=3, stride=1, padding='SAME'), \n",
        "      jax.nn.relu, \n",
        "      hk.MaxPool(window_shape=(1,2,2,1), strides=(1,1,1,1), padding='VALID'),\n",
        "      # block 3\n",
        "      hk.Conv2D(64, kernel_shape=3, stride=1, padding='SAME'), \n",
        "      jax.nn.relu, \n",
        "      hk.MaxPool(window_shape=(1,2,2,1), strides=(1,1,1,1), padding='VALID'),\n",
        "      # block 4\n",
        "      hk.Conv2D(32, kernel_shape=3, stride=1, padding='SAME')\n",
        "    ])\n",
        "\n",
        "    self.mlp_stage = hk.Sequential([\n",
        "      hk.Flatten(),\n",
        "      hk.Linear(128), \n",
        "      jax.nn.relu, \n",
        "      hk.Linear(n_classes)\n",
        "    ])\n",
        "\n",
        "    self.p_dropout = dropout\n",
        "\n",
        "  def __call__(self, x, use_dropout=True):\n",
        "    x = self.conv_stage(x)\n",
        "    \n",
        "    dropout_rate = self.p_dropout if use_dropout else 0.0\n",
        "    x = hk.dropout(hk.next_rng_key(), dropout_rate, x)\n",
        "\n",
        "    return self.mlp_stage(x)\n",
        "\n",
        "# standard normalization constants\n",
        "mean_norm = jnp.array([[0.4914, 0.4822, 0.4465]])\n",
        "std_norm = jnp.array([[0.247, 0.243, 0.261]])\n",
        "\n",
        "#define the net-function \n",
        "def net_fn(batch, use_dropout):\n",
        "  net = Net(dropout=0.0)\n",
        "  x = batch['image']/255.0\n",
        "  x = (x - mean_norm) / std_norm\n",
        "  return net(x, use_dropout)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eo-Gypdbo0wY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hyperparameters\n",
        "lr = 1e-3\n",
        "reg = 1e-4\n",
        "\n",
        "# instantiate the network\n",
        "net = hk.transform(net_fn)\n",
        "\n",
        "# build the optimizer\n",
        "opt_init, opt_update, opt_get_params = optimizers.rmsprop(lr)\n",
        "\n",
        "# standard L2-regularized crossentropy loss function\n",
        "def loss(params, rng, batch):\n",
        "    logits = net.apply(params, rng, batch, use_dropout=True)\n",
        "    labels = jax.nn.one_hot(batch['label'], 10)\n",
        "\n",
        "    l2_loss = 0.5 * sum(jnp.sum(jnp.square(p)) \n",
        "                        for p in jax.tree_leaves(params))\n",
        "    softmax_crossent = - jnp.mean(labels * jax.nn.log_softmax(logits))\n",
        "\n",
        "    return softmax_crossent + reg * l2_loss\n",
        "\n",
        "@jax.jit\n",
        "def accuracy(params, batch):\n",
        "  preds = net.apply(params, jax.random.PRNGKey(101), batch, use_dropout=False)\n",
        "  return jnp.mean(jnp.argmax(preds, axis=-1) == batch['label'])\n",
        "\n",
        "@jax.jit\n",
        "def train_step(i, opt_state, rng, batch):\n",
        "\tparams = opt_get_params(opt_state)\n",
        "\tfx, dx = jax.value_and_grad(loss)(params, rng, batch)\n",
        "\topt_state = opt_update(i, dx, opt_state)\n",
        "\treturn fx, opt_state"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfv4Mldkdt40",
        "colab_type": "text"
      },
      "source": [
        "## Load the Initialization, Val and Test Batches & Do the Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYeEembgpgSk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "init_batches = load_dataset(\"train\", is_training=True, batch_size=256)\n",
        "val_batches = load_dataset(\"train\", is_training=False, batch_size=1_000)\n",
        "test_batches = load_dataset(\"test\", is_training=False, batch_size=1_000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MDzIu4uxmeD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "240f3443-5529-4e12-9ad8-7af870738e3d"
      },
      "source": [
        "%%time\n",
        "\n",
        "# intialize the paramaeters\n",
        "params = net.init(jax.random.PRNGKey(42), next(init_batches), use_dropout=True)\n",
        "opt_state = opt_init(params)\n",
        "\n",
        "# initialize a key for the dropout\n",
        "rng = jax.random.PRNGKey(2)\n",
        "\n",
        "for epoch in range(100):\n",
        "\t #generate a shuffled epoch of training data\n",
        "  train_batches = load_dataset(\"train\", is_training=True,\n",
        "                              batch_size=256, repeat=False, seed=epoch)\n",
        "  \n",
        "  for batch in train_batches:\n",
        "    # run an optimization step\n",
        "    train_loss, opt_state = train_step(epoch, opt_state, rng, batch)\n",
        "    \n",
        "    # make more rng for the dropout\n",
        "    rng, _ = jax.random.split(rng)\n",
        "\t\n",
        "  if epoch % 5 == 0:\n",
        "    params = opt_get_params(opt_state)\n",
        "    val_acc = accuracy(params, next(val_batches))\n",
        "    test_acc = accuracy(params, next(test_batches))\n",
        "    print(f\"epoch = {epoch}\"\n",
        "          f\" | train loss = {train_loss:.4f}\"\n",
        "          f\" | val acc = {val_acc:.3f}\"\n",
        "          f\" | test acc = {test_acc:.3f}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch = 0 | train loss = 0.1405 | val acc = 0.489 | test acc = 0.515\n",
            "epoch = 5 | train loss = 0.0659 | val acc = 0.788 | test acc = 0.688\n",
            "epoch = 10 | train loss = 0.0596 | val acc = 0.818 | test acc = 0.669\n",
            "epoch = 15 | train loss = 0.0554 | val acc = 0.896 | test acc = 0.702\n",
            "epoch = 20 | train loss = 0.0598 | val acc = 0.880 | test acc = 0.646\n",
            "epoch = 25 | train loss = 0.0547 | val acc = 0.939 | test acc = 0.709\n",
            "epoch = 30 | train loss = 0.0504 | val acc = 0.966 | test acc = 0.714\n",
            "epoch = 35 | train loss = 0.0502 | val acc = 0.953 | test acc = 0.705\n",
            "epoch = 40 | train loss = 0.0637 | val acc = 0.954 | test acc = 0.723\n",
            "epoch = 45 | train loss = 0.0494 | val acc = 0.957 | test acc = 0.718\n",
            "epoch = 50 | train loss = 0.0472 | val acc = 0.952 | test acc = 0.731\n",
            "epoch = 55 | train loss = 0.0458 | val acc = 0.972 | test acc = 0.717\n",
            "epoch = 60 | train loss = 0.0503 | val acc = 0.952 | test acc = 0.730\n",
            "epoch = 65 | train loss = 0.0490 | val acc = 0.962 | test acc = 0.705\n",
            "epoch = 70 | train loss = 0.0554 | val acc = 0.959 | test acc = 0.695\n",
            "epoch = 75 | train loss = 0.0488 | val acc = 0.973 | test acc = 0.716\n",
            "epoch = 80 | train loss = 0.0479 | val acc = 0.976 | test acc = 0.726\n",
            "epoch = 85 | train loss = 0.0499 | val acc = 0.963 | test acc = 0.728\n",
            "epoch = 90 | train loss = 0.0565 | val acc = 0.947 | test acc = 0.722\n",
            "epoch = 95 | train loss = 0.0491 | val acc = 0.963 | test acc = 0.725\n",
            "CPU times: user 30min 24s, sys: 11min 53s, total: 42min 17s\n",
            "Wall time: 23min 15s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}