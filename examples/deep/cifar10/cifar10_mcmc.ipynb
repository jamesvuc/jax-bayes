{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar10_mcmc.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuajMtOoy3xC",
        "colab_type": "text"
      },
      "source": [
        "# jax-bayes CIFAR10 Example --- Bayesian MCMC Approach\n",
        "\n",
        "## Set Up the Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfMSaNHlceB7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 795
        },
        "outputId": "ad97595e-f01d-490d-c9e1-51061a76cdf5"
      },
      "source": [
        "#see https://github.com/google/jax#pip-installation\n",
        "!pip install --upgrade https://storage.googleapis.com/jax-releases/cuda101/jaxlib-0.1.51-cp36-none-manylinux2010_x86_64.whl\n",
        "!pip install --upgrade jax\n",
        "!pip install git+https://github.com/deepmind/dm-haiku\n",
        "!pip install git+https://github.com/jamesvuc/jax-bayes"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting jaxlib==0.1.51\n",
            "  Using cached https://storage.googleapis.com/jax-releases/cuda101/jaxlib-0.1.51-cp36-none-manylinux2010_x86_64.whl\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from jaxlib==0.1.51) (1.5.2)\n",
            "Requirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.6/dist-packages (from jaxlib==0.1.51) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from jaxlib==0.1.51) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from absl-py->jaxlib==0.1.51) (1.15.0)\n",
            "Installing collected packages: jaxlib\n",
            "  Found existing installation: jaxlib 0.1.51\n",
            "    Uninstalling jaxlib-0.1.51:\n",
            "      Successfully uninstalled jaxlib-0.1.51\n",
            "Successfully installed jaxlib-0.1.51\n",
            "Requirement already up-to-date: jax in /usr/local/lib/python3.6/dist-packages (0.1.75)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from jax) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.6/dist-packages (from jax) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum in /usr/local/lib/python3.6/dist-packages (from jax) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from absl-py->jax) (1.15.0)\n",
            "Collecting git+https://github.com/deepmind/dm-haiku\n",
            "  Cloning https://github.com/deepmind/dm-haiku to /tmp/pip-req-build-yf2rs_hb\n",
            "  Running command git clone -q https://github.com/deepmind/dm-haiku /tmp/pip-req-build-yf2rs_hb\n",
            "Requirement already satisfied (use --upgrade to upgrade): dm-haiku==0.0.2 from git+https://github.com/deepmind/dm-haiku in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from dm-haiku==0.0.2) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.6/dist-packages (from dm-haiku==0.0.2) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py>=0.7.1->dm-haiku==0.0.2) (1.15.0)\n",
            "Building wheels for collected packages: dm-haiku\n",
            "  Building wheel for dm-haiku (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dm-haiku: filename=dm_haiku-0.0.2-cp36-none-any.whl size=289739 sha256=3b8458b694f0318292ff7f1ef1f8a08f8166e3141772ec2afc50f2464a55d1b0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-nu9w8nn8/wheels/97/0f/e9/17f34e377f8d4060fa88a7e82bee5d8afbf7972384768a5499\n",
            "Successfully built dm-haiku\n",
            "Collecting git+https://github.com/jamesvuc/jax-bayes\n",
            "  Cloning https://github.com/jamesvuc/jax-bayes to /tmp/pip-req-build-2qkv8e8a\n",
            "  Running command git clone -q https://github.com/jamesvuc/jax-bayes /tmp/pip-req-build-2qkv8e8a\n",
            "Requirement already satisfied (use --upgrade to upgrade): jax-bayes==0.0.1 from git+https://github.com/jamesvuc/jax-bayes in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: absl-py>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from jax-bayes==0.0.1) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.6/dist-packages (from jax-bayes==0.0.1) (1.18.5)\n",
            "Requirement already satisfied: opt-einsum>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from jax-bayes==0.0.1) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.12.4 in /usr/local/lib/python3.6/dist-packages (from jax-bayes==0.0.1) (3.12.4)\n",
            "Requirement already satisfied: scipy>=1.5.2 in /usr/local/lib/python3.6/dist-packages (from jax-bayes==0.0.1) (1.5.2)\n",
            "Requirement already satisfied: six>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from jax-bayes==0.0.1) (1.15.0)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.6/dist-packages (from jax-bayes==0.0.1) (4.48.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.12.4->jax-bayes==0.0.1) (49.2.0)\n",
            "Building wheels for collected packages: jax-bayes\n",
            "  Building wheel for jax-bayes (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jax-bayes: filename=jax_bayes-0.0.1-cp36-none-any.whl size=1009734 sha256=eddebc139a0a210a3d1cbe62944d13d03eb84b8574fa2ade56d50e429f9cb824\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5kvj05ns/wheels/31/65/d6/bcf4b5e84c6f6f176e73850145875e806569759c23081b4446\n",
            "Successfully built jax-bayes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l70DSI0ajQJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import haiku as hk\n",
        "\n",
        "import jax.numpy as jnp\n",
        "from jax.experimental import optimizers\n",
        "import jax\n",
        "\n",
        "import jax_bayes\n",
        "\n",
        "import sys, os, math, time\n",
        "import numpy as np\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B686kNdCzFEP",
        "colab_type": "text"
      },
      "source": [
        "## Build the dataset loader and CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzgaUa2owIqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset(split, is_training, batch_size, repeat=True, seed=0):\n",
        "  if repeat:\n",
        "    ds = tfds.load('cifar10', split=split).cache().repeat()\n",
        "  else:\n",
        "    ds = tfds.load('cifar10', split=split).cache()\n",
        "  if is_training:\n",
        "    ds = ds.shuffle(10 * batch_size, seed=seed)\n",
        "  ds = ds.batch(batch_size)\n",
        "  return tfds.as_numpy(ds)\n",
        "\n",
        "# build a 32-32-64-32 CNN with max-pooling \n",
        "# followed by a 128-10-n_classes MLP\n",
        "class Net(hk.Module):\n",
        "  def __init__(self, dropout=0.1, n_classes=10):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv_stage = hk.Sequential([\n",
        "      #block 1\n",
        "      hk.Conv2D(32, kernel_shape=3, stride=1, padding='SAME'), \n",
        "      jax.nn.relu, \n",
        "      hk.MaxPool(window_shape=(1,2,2,1), strides=(1,1,1,1), padding='VALID'),\n",
        "      # block 2\n",
        "      hk.Conv2D(32, kernel_shape=3, stride=1, padding='SAME'), \n",
        "      jax.nn.relu, \n",
        "      hk.MaxPool(window_shape=(1,2,2,1), strides=(1,1,1,1), padding='VALID'),\n",
        "      # block 3\n",
        "      hk.Conv2D(64, kernel_shape=3, stride=1, padding='SAME'), \n",
        "      jax.nn.relu, \n",
        "      hk.MaxPool(window_shape=(1,2,2,1), strides=(1,1,1,1), padding='VALID'),\n",
        "      # block 4\n",
        "      hk.Conv2D(32, kernel_shape=3, stride=1, padding='SAME')\n",
        "    ])\n",
        "\n",
        "    self.mlp_stage = hk.Sequential([\n",
        "      hk.Flatten(),\n",
        "      hk.Linear(128), \n",
        "      jax.nn.relu, \n",
        "      hk.Linear(n_classes)\n",
        "    ])\n",
        "\n",
        "    self.p_dropout = dropout\n",
        "\n",
        "  def __call__(self, x, use_dropout=True):\n",
        "    x = self.conv_stage(x)\n",
        "    \n",
        "    dropout_rate = self.p_dropout if use_dropout else 0.0\n",
        "    x = hk.dropout(hk.next_rng_key(), dropout_rate, x)\n",
        "\n",
        "    return self.mlp_stage(x)\n",
        "\n",
        "# standard normalization constants\n",
        "mean_norm = jnp.array([[0.4914, 0.4822, 0.4465]])\n",
        "std_norm = jnp.array([[0.247, 0.243, 0.261]])\n",
        "\n",
        "#define the net-function \n",
        "def net_fn(batch, use_dropout):\n",
        "  net = Net(dropout=0.0)\n",
        "  x = batch['image']/255.0\n",
        "  x = (x - mean_norm) / std_norm\n",
        "  return net(x, use_dropout)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jAvwWBY6D9Q",
        "colab_type": "text"
      },
      "source": [
        "## Build the Loss, Metrics, and MCMC step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZva_QuKwO_2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hyperparameters\n",
        "# lr = 1e-2\n",
        "lr_initial = 1e-2\n",
        "lr_final = 1e-3\n",
        "decay_start = 100\n",
        "decay_steps = 100\n",
        "decay_schedule = jax.experimental.optimizers.polynomial_decay(lr_initial, decay_steps, lr_final, power=1.0)\n",
        "lr = lambda t: jax.lax.cond(t < decay_start,\n",
        "                            lambda s: lr_initial,\n",
        "                            lambda s: decay_schedule(s - decay_start),\n",
        "                            t)\n",
        "\n",
        "\n",
        "reg = 1e-4\n",
        "num_samples = 5\n",
        "#for this example, we're going to use the jax initializers to sample the initial \n",
        "# distribution, so we will use init_stddev = 0.0\n",
        "init_stddev = 0.0 \n",
        "\n",
        "# instantiate the network\n",
        "net = hk.transform(net_fn)\n",
        "\n",
        "# build the sampler\n",
        "key = jax.random.PRNGKey(0)\n",
        "sampler_init, sampler_propose, sampler_update, sampler_get_params = \\\n",
        "  jax_bayes.mcmc.rms_langevin_fns(key, num_samples=-1, step_size=lr, \n",
        "                                  init_stddev=init_stddev)\n",
        "\n",
        "# standard regularized crossentropy loss function, which is the \n",
        "# negative unnormalized log-posterior \n",
        "def loss(params, rng, batch):\n",
        "    logits = net.apply(params, rng, batch, use_dropout=True)\n",
        "    labels = jax.nn.one_hot(batch['label'], 10)\n",
        "\n",
        "    l2_loss = 0.5 * sum(jnp.sum(jnp.square(p)) \n",
        "                        for p in jax.tree_leaves(params))\n",
        "    softmax_crossent = - jnp.mean(labels * jax.nn.log_softmax(logits))\n",
        "\n",
        "    return softmax_crossent + reg * l2_loss\n",
        "\n",
        "logprob = lambda p,k,b : - loss(p, k, b)\n",
        "\n",
        "@jax.jit\n",
        "def accuracy(params, batch):\n",
        "  pred_fn = lambda p:net.apply(p, jax.random.PRNGKey(101), batch, use_dropout=False)\n",
        "  pred_fn = jax.vmap(pred_fn)\n",
        "  preds = jnp.mean(pred_fn(params), axis=0)\n",
        "  return jnp.mean(jnp.argmax(preds, axis=-1) == batch['label'])\n",
        "\n",
        "# the data loss will help us monitor the Markov chain's progress without worrying\n",
        "# about the effects of regularization.\n",
        "def data_loss(params, batch):\n",
        "    logits = net.apply(params, jax.random.PRNGKey(0), batch, use_dropout=False)\n",
        "    labels = jax.nn.one_hot(batch['label'], 10)\n",
        "    softmax_crossent = - jnp.mean(labels * jax.nn.log_softmax(logits))\n",
        "    return softmax_crossent\n",
        "data_loss = jax.vmap(data_loss, in_axes=(0, None))\n",
        "\n",
        "@jax.jit\n",
        "def mcmc_step(i, sampler_state, sampler_keys, rng, batch):\n",
        "  params = sampler_get_params(sampler_state)\n",
        "  logp = lambda p,k: logprob(p, k, batch)\n",
        "  fx, dx = jax.vmap(jax.value_and_grad(logp))(params, rng)\n",
        "\n",
        "  sampler_prop_state, new_keys = sampler_propose(i, dx, sampler_state, \n",
        "                                        sampler_keys)\n",
        "\n",
        "  fx_prop, dx_prop = fx, dx\n",
        "\n",
        "  sampler_state, new_keys = sampler_update(i, fx, fx_prop, \n",
        "                        dx, sampler_state, \n",
        "                        dx_prop, sampler_prop_state, \n",
        "                        new_keys)\n",
        "  \n",
        "  return jnp.mean(fx), sampler_state, new_keys"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDH0DnMy7Zc_",
        "colab_type": "text"
      },
      "source": [
        "## Load Batch iterators & Do the MCMC inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTAPjsUi8lJF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "init_batches = load_dataset(\"train\", is_training=True, batch_size=512)\n",
        "val_batches = load_dataset(\"train\", is_training=False, batch_size=2_000)\n",
        "test_batches = load_dataset(\"test\", is_training=False, batch_size=2_000)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJPiQfaRGWAc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 881
        },
        "outputId": "0d7df7bc-9807-4928-ea80-c707897b92eb"
      },
      "source": [
        "#Use the vmap-over-keys trick to sample a highly anisotropic initial distribution\n",
        "init_batch = next(init_batches)\n",
        "keys = jax.random.split(jax.random.PRNGKey(1), num_samples)\n",
        "init_param_samples = jax.vmap(lambda k:net.init(k, init_batch, use_dropout=True))(keys)\n",
        "sampler_state, sampler_keys = sampler_init(init_param_samples)\n",
        "\n",
        "# generate RNGs for the dropout\n",
        "rngs = jax.random.split(jax.random.PRNGKey(2), num_samples)\n",
        "\n",
        "for epoch in range(250):\n",
        "  #generate a shuffled epoch of training data\n",
        "  train_batches = load_dataset(\"train\", is_training=True,\n",
        "                              batch_size=128, repeat=False, seed=epoch)\n",
        "  \n",
        "  start = time.time()\n",
        "  for batch in train_batches:\n",
        "    # run an MCMC step\n",
        "    train_logprob, sampler_state, sampler_keys = \\\n",
        "      mcmc_step(epoch, sampler_state, sampler_keys, rngs, batch)\n",
        "    \n",
        "    # make more rngs for the dropout\n",
        "    rngs = jax.random.split(rngs[0], num_samples)\n",
        "  epoch_time = time.time() - start\n",
        "\n",
        "  if epoch % 5 == 0:\n",
        "    # compute val and test accuracy, and the sampler-average data loss\n",
        "    params = sampler_get_params(sampler_state)\n",
        "    val_acc = accuracy(params, next(val_batches))\n",
        "    test_acc = accuracy(params, next(test_batches))\n",
        "    _data_loss = jnp.mean(data_loss(params, next(val_batches)))\n",
        "    print(f\"epoch = {epoch}\"\n",
        "        f\" | time per epoch {epoch_time:.3f}\"\n",
        "        f\" | data loss = {_data_loss:.3e}\"\n",
        "        f\" | val acc = {val_acc:.3f}\"\n",
        "        f\" | test acc = {test_acc:.3f}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch = 0 | time per epoch 43.130 | data loss = 1.126e+17 | val acc = 0.189 | test acc = 0.193\n",
            "epoch = 5 | time per epoch 35.306 | data loss = 1.354e+15 | val acc = 0.333 | test acc = 0.346\n",
            "epoch = 10 | time per epoch 35.097 | data loss = 3.986e+14 | val acc = 0.351 | test acc = 0.350\n",
            "epoch = 15 | time per epoch 34.981 | data loss = 1.929e+14 | val acc = 0.388 | test acc = 0.368\n",
            "epoch = 20 | time per epoch 34.972 | data loss = 1.142e+14 | val acc = 0.392 | test acc = 0.399\n",
            "epoch = 25 | time per epoch 34.980 | data loss = 7.258e+13 | val acc = 0.412 | test acc = 0.413\n",
            "epoch = 30 | time per epoch 34.928 | data loss = 5.112e+13 | val acc = 0.438 | test acc = 0.399\n",
            "epoch = 35 | time per epoch 34.901 | data loss = 3.738e+13 | val acc = 0.442 | test acc = 0.416\n",
            "epoch = 40 | time per epoch 34.915 | data loss = 3.082e+13 | val acc = 0.455 | test acc = 0.404\n",
            "epoch = 45 | time per epoch 34.886 | data loss = 2.601e+13 | val acc = 0.458 | test acc = 0.432\n",
            "epoch = 50 | time per epoch 34.899 | data loss = 1.859e+13 | val acc = 0.483 | test acc = 0.446\n",
            "epoch = 55 | time per epoch 34.876 | data loss = 1.997e+13 | val acc = 0.508 | test acc = 0.429\n",
            "epoch = 60 | time per epoch 34.876 | data loss = 1.398e+13 | val acc = 0.479 | test acc = 0.422\n",
            "epoch = 65 | time per epoch 34.840 | data loss = 1.359e+13 | val acc = 0.470 | test acc = 0.416\n",
            "epoch = 70 | time per epoch 34.826 | data loss = 1.226e+13 | val acc = 0.465 | test acc = 0.417\n",
            "epoch = 75 | time per epoch 34.958 | data loss = 9.665e+12 | val acc = 0.503 | test acc = 0.447\n",
            "epoch = 80 | time per epoch 34.830 | data loss = 1.243e+13 | val acc = 0.495 | test acc = 0.441\n",
            "epoch = 85 | time per epoch 34.898 | data loss = 6.606e+12 | val acc = 0.483 | test acc = 0.426\n",
            "epoch = 90 | time per epoch 34.837 | data loss = 6.185e+12 | val acc = 0.475 | test acc = 0.400\n",
            "epoch = 95 | time per epoch 34.866 | data loss = 4.987e+12 | val acc = 0.524 | test acc = 0.454\n",
            "epoch = 100 | time per epoch 34.846 | data loss = 6.212e+12 | val acc = 0.478 | test acc = 0.428\n",
            "epoch = 105 | time per epoch 34.804 | data loss = 4.210e+12 | val acc = 0.507 | test acc = 0.445\n",
            "epoch = 110 | time per epoch 34.832 | data loss = 3.411e+12 | val acc = 0.501 | test acc = 0.437\n",
            "epoch = 115 | time per epoch 34.879 | data loss = 4.890e+12 | val acc = 0.446 | test acc = 0.396\n",
            "epoch = 120 | time per epoch 34.803 | data loss = 3.994e+12 | val acc = 0.522 | test acc = 0.451\n",
            "epoch = 125 | time per epoch 34.815 | data loss = 4.236e+12 | val acc = 0.532 | test acc = 0.472\n",
            "epoch = 130 | time per epoch 34.806 | data loss = 3.272e+12 | val acc = 0.514 | test acc = 0.445\n",
            "epoch = 135 | time per epoch 34.820 | data loss = 4.463e+12 | val acc = 0.534 | test acc = 0.444\n",
            "epoch = 140 | time per epoch 34.740 | data loss = 2.526e+12 | val acc = 0.548 | test acc = 0.456\n",
            "epoch = 145 | time per epoch 34.780 | data loss = 1.986e+12 | val acc = 0.532 | test acc = 0.456\n",
            "epoch = 150 | time per epoch 34.808 | data loss = 2.967e+12 | val acc = 0.568 | test acc = 0.473\n",
            "epoch = 155 | time per epoch 34.785 | data loss = 2.285e+12 | val acc = 0.550 | test acc = 0.432\n",
            "epoch = 160 | time per epoch 34.759 | data loss = 3.253e+12 | val acc = 0.538 | test acc = 0.439\n",
            "epoch = 165 | time per epoch 34.752 | data loss = 1.835e+12 | val acc = 0.573 | test acc = 0.462\n",
            "epoch = 170 | time per epoch 34.803 | data loss = 2.220e+12 | val acc = 0.574 | test acc = 0.465\n",
            "epoch = 175 | time per epoch 34.823 | data loss = 1.506e+12 | val acc = 0.576 | test acc = 0.476\n",
            "epoch = 180 | time per epoch 34.855 | data loss = 2.284e+12 | val acc = 0.599 | test acc = 0.448\n",
            "epoch = 185 | time per epoch 34.823 | data loss = 2.918e+12 | val acc = 0.573 | test acc = 0.464\n",
            "epoch = 190 | time per epoch 34.790 | data loss = 2.949e+12 | val acc = 0.551 | test acc = 0.454\n",
            "epoch = 195 | time per epoch 34.758 | data loss = 2.196e+12 | val acc = 0.575 | test acc = 0.457\n",
            "epoch = 200 | time per epoch 34.800 | data loss = 2.651e+12 | val acc = 0.576 | test acc = 0.469\n",
            "epoch = 205 | time per epoch 34.862 | data loss = 3.374e+12 | val acc = 0.562 | test acc = 0.443\n",
            "epoch = 210 | time per epoch 34.877 | data loss = 3.457e+12 | val acc = 0.586 | test acc = 0.459\n",
            "epoch = 215 | time per epoch 34.841 | data loss = 2.016e+12 | val acc = 0.603 | test acc = 0.463\n",
            "epoch = 220 | time per epoch 34.778 | data loss = 1.775e+12 | val acc = 0.589 | test acc = 0.456\n",
            "epoch = 225 | time per epoch 34.809 | data loss = 1.632e+12 | val acc = 0.607 | test acc = 0.477\n",
            "epoch = 230 | time per epoch 34.948 | data loss = 1.829e+12 | val acc = 0.575 | test acc = 0.448\n",
            "epoch = 235 | time per epoch 34.810 | data loss = 1.301e+12 | val acc = 0.571 | test acc = 0.457\n",
            "epoch = 240 | time per epoch 34.725 | data loss = 1.250e+12 | val acc = 0.586 | test acc = 0.448\n",
            "epoch = 245 | time per epoch 34.781 | data loss = 1.900e+12 | val acc = 0.592 | test acc = 0.451\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVmiAoypRMOP",
        "colab_type": "text"
      },
      "source": [
        "**Note**: This example highlights how Bayesian ML and regular ML are very different. \n",
        "\n",
        "- We know a lot less about efficient inference than we do optimization.\n",
        "- Accuracy of around 45% (vs 70% for the optimization approach) is only a bit worse than current SoTA algorithms for this architecture (see e.g. [This paper](https://arxiv.org/pdf/1709.01180.pdf)). More hyperparameter tuning could probably close this gap.\n",
        "- In fact many MCMC papers do not evaluate on CIFAR10 (preferring to use MNIST, where we can easily achieve >96%)\n",
        "- There are several factors that contribute to MCMC's increased difficulty:\n",
        "  - stochastic gradients\n",
        "  - dependence on hyperparameters\n",
        "  - regularization techniques\n",
        "  - probabilistic algorithms are generally more subtle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83Sp_OeSLBf_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e4c03c7a-fa70-4fec-c631-d8f8d6672aec"
      },
      "source": [
        "def posterior_predictive(params, batch):\n",
        "  \"\"\"computes the posterior_predictive P(class = c | inputs, params) using a histogram\n",
        "  \"\"\"\n",
        "  pred_fn = lambda p:net.apply(p, jax.random.PRNGKey(0), batch, use_dropout=False) \n",
        "  pred_fn = jax.vmap(pred_fn)\n",
        "\n",
        "  logit_samples = pred_fn(params) # n_samples x batch_size x n_classes\n",
        "  pred_samples = jnp.argmax(logit_samples, axis=-1) #n_samples x batch_size\n",
        "\n",
        "  n_classes = logit_samples.shape[-1]\n",
        "  batch_size = logit_samples.shape[1]\n",
        "  probs = np.zeros((batch_size, n_classes))\n",
        "  for c in range(n_classes):\n",
        "    idxs = pred_samples == c\n",
        "    probs[:,c] = idxs.sum(axis=0)\n",
        "\n",
        "  return probs / probs.sum(axis=1, keepdims=True)\n",
        "\n",
        "params = sampler_get_params(sampler_state)\n",
        "print('Final predictive entropy', jnp.mean(jax_bayes.utils.entropy(posterior_predictive(params, next(test_batches)))))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final predictive entropy 1.3844115\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}